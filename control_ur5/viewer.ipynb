{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手腕坐标 (图像空间): 367 661 1173.0\n",
      "手腕坐标 (图像空间): 367 661 1177.0\n",
      "手腕坐标 (图像空间): 368 661 1172.0\n",
      "手腕坐标 (图像空间): 369 660 1171.0\n",
      "手腕坐标 (图像空间): 368 659 1173.0\n",
      "手腕坐标 (图像空间): 516 681 749.0\n",
      "手腕坐标 (图像空间): 562 678 720.0\n",
      "手腕坐标 (图像空间): 547 654 715.0\n",
      "手腕坐标 (图像空间): 587 619 729.0\n",
      "手腕坐标 (图像空间): 597 601 734.0\n",
      "手腕坐标 (图像空间): 601 581 738.0\n",
      "手腕坐标 (图像空间): 603 536 745.0\n",
      "手腕坐标 (图像空间): 616 548 756.0\n",
      "手腕坐标 (图像空间): 628 512 772.0\n",
      "手腕坐标 (图像空间): 630 493 779.0\n",
      "手腕坐标 (图像空间): 632 478 789.0\n",
      "手腕坐标 (图像空间): 641 457 817.0\n",
      "手腕坐标 (图像空间): 643 449 827.0\n",
      "手腕坐标 (图像空间): 646 442 836.0\n",
      "手腕坐标 (图像空间): 653 431 861.0\n",
      "手腕坐标 (图像空间): 655 425 879.0\n",
      "手腕坐标 (图像空间): 662 410 897.0\n",
      "手腕坐标 (图像空间): 666 402 913.0\n",
      "手腕坐标 (图像空间): 670 395 923.0\n",
      "手腕坐标 (图像空间): 690 373 934.0\n",
      "手腕坐标 (图像空间): 706 362 940.0\n",
      "手腕坐标 (图像空间): 734 349 964.0\n",
      "手腕坐标 (图像空间): 731 352 967.0\n",
      "手腕坐标 (图像空间): 740 349 978.0\n",
      "手腕坐标 (图像空间): 747 347 986.0\n",
      "手腕坐标 (图像空间): 752 347 997.0\n",
      "手腕坐标 (图像空间): 758 346 1025.0\n",
      "手腕坐标 (图像空间): 761 346 1038.0\n",
      "手腕坐标 (图像空间): 763 344 1047.0\n",
      "手腕坐标 (图像空间): 764 342 1053.0\n",
      "手腕坐标 (图像空间): 764 341 1056.0\n",
      "手腕坐标 (图像空间): 765 340 1068.0\n",
      "手腕坐标 (图像空间): 765 339 1069.0\n",
      "手腕坐标 (图像空间): 765 339 1068.0\n",
      "手腕坐标 (图像空间): 764 339 1066.0\n",
      "手腕坐标 (图像空间): 764 340 1067.0\n",
      "手腕坐标 (图像空间): 763 340 1062.0\n",
      "手腕坐标 (图像空间): 763 341 1071.0\n",
      "手腕坐标 (图像空间): 762 341 1060.0\n",
      "手腕坐标 (图像空间): 762 341 1063.0\n",
      "手腕坐标 (图像空间): 762 341 1064.0\n",
      "手腕坐标 (图像空间): 760 341 1056.0\n",
      "手腕坐标 (图像空间): 756 341 1060.0\n",
      "手腕坐标 (图像空间): 732 343 1062.0\n",
      "手腕坐标 (图像空间): 700 342 1059.0\n",
      "手腕坐标 (图像空间): 709 349 1063.0\n",
      "手腕坐标 (图像空间): 620 378 1081.0\n",
      "手腕坐标 (图像空间): 562 426 1107.0\n",
      "手腕坐标 (图像空间): 549 440 1108.0\n",
      "手腕坐标 (图像空间): 536 456 1119.0\n",
      "手腕坐标 (图像空间): 534 460 1122.0\n",
      "手腕坐标 (图像空间): 536 460 1118.0\n",
      "手腕坐标 (图像空间): 554 446 1084.0\n",
      "手腕坐标 (图像空间): 555 448 1086.0\n",
      "手腕坐标 (图像空间): 575 432 1051.0\n",
      "手腕坐标 (图像空间): 616 405 998.0\n",
      "手腕坐标 (图像空间): 785 373 965.0\n",
      "手腕坐标 (图像空间): 798 379 984.0\n",
      "手腕坐标 (图像空间): 777 378 980.0\n",
      "手腕坐标 (图像空间): 746 376 982.0\n",
      "手腕坐标 (图像空间): 627 388 1028.0\n",
      "手腕坐标 (图像空间): 577 411 1063.0\n",
      "手腕坐标 (图像空间): 552 450 1106.0\n",
      "手腕坐标 (图像空间): 538 500 1146.0\n",
      "手腕坐标 (图像空间): 538 497 1119.0\n",
      "手腕坐标 (图像空间): 585 437 1020.0\n",
      "手腕坐标 (图像空间): 645 405 963.0\n",
      "手腕坐标 (图像空间): 802 383 952.0\n",
      "手腕坐标 (图像空间): 849 391 974.0\n",
      "手腕坐标 (图像空间): 861 402 1002.0\n",
      "手腕坐标 (图像空间): 843 401 1000.0\n",
      "手腕坐标 (图像空间): 813 393 1009.0\n",
      "手腕坐标 (图像空间): 720 383 1029.0\n",
      "手腕坐标 (图像空间): 672 392 1052.0\n",
      "手腕坐标 (图像空间): 635 413 1080.0\n",
      "手腕坐标 (图像空间): 591 456 1109.0\n",
      "手腕坐标 (图像空间): 585 468 1110.0\n",
      "手腕坐标 (图像空间): 583 473 1089.0\n",
      "手腕坐标 (图像空间): 597 470 1064.0\n",
      "手腕坐标 (图像空间): 611 467 1044.0\n",
      "手腕坐标 (图像空间): 650 472 1007.0\n",
      "手腕坐标 (图像空间): 625 459 1013.0\n",
      "手腕坐标 (图像空间): 638 454 916.0\n",
      "手腕坐标 (图像空间): 692 492 964.0\n",
      "手腕坐标 (图像空间): 748 537 892.0\n",
      "手腕坐标 (图像空间): 741 543 883.0\n",
      "手腕坐标 (图像空间): 777 587 930.0\n",
      "手腕坐标 (图像空间): 779 653 931.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# adopt rgb-d to read information\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# 创建一个管道对象\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# 创建一个配置对象\n",
    "config = rs.config()\n",
    "\n",
    "# 配置RGB和深度流，设置最大的分辨率和帧率\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30) \n",
    "\n",
    "# 开始流\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    try:\n",
    "        while True:\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            aligned_frames = align.process(frames)\n",
    "\n",
    "            # 获取对齐后的深度和颜色帧\n",
    "            aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "            color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "            if not aligned_depth_frame or not color_frame:\n",
    "                continue\n",
    "\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            depth_image = np.asanyarray(aligned_depth_frame.get_data())\n",
    "\n",
    "            # 将图像从 BGR 转换为 RGB\n",
    "            color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "            color_image.flags.writeable = False\n",
    "            results = hands.process(color_image)\n",
    "\n",
    "            color_image.flags.writeable = True\n",
    "            color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        color_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    # 打印手腕坐标\n",
    "                    wrist_landmark = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                    wrist_x = min(int(wrist_landmark.x * color_image.shape[1]), color_image.shape[1] - 1)\n",
    "                    wrist_y = min(int(wrist_landmark.y * color_image.shape[0]), color_image.shape[0] - 1)\n",
    "                    wrist_z = depth_image[wrist_y, wrist_x].astype(float)\n",
    "                    print(\"手腕坐标 (图像空间):\", wrist_x, wrist_y, wrist_z)\n",
    "\n",
    "            cv2.imshow('MediaPipe Hands', color_image)\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        pipeline.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable demo backup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from pyrealsense2 import pyrealsense2 as rs\n",
    "from controller import Robot\n",
    "from ikpy.chain import Chain\n",
    "\n",
    "# for testing performance\n",
    "import pandas as pd\n",
    "original = list()\n",
    "after = list()\n",
    "\n",
    "# Mediapipe相关设置\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)  # 初始化手部检测模型\n",
    "\n",
    "# 设置realsense相机\n",
    "pipe = rs.pipeline()\n",
    "config = rs.config()\n",
    "# config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "# config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "profile = pipe.start(config)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "\n",
    "# 获取相机内参\n",
    "intr = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()\n",
    "\n",
    "\n",
    "# 定义一个模拟的UR5机器人类\n",
    "class myUR5(Robot):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.timestep = int(self.getBasicTimeStep())\n",
    "\n",
    "        # 获取机器人的电机\n",
    "        self.motors = [\n",
    "            self.getDevice(\"shoulder_pan_joint\"),\n",
    "            self.getDevice(\"shoulder_lift_joint\"),\n",
    "            self.getDevice(\"elbow_joint\"),\n",
    "            self.getDevice(\"wrist_1_joint\"),\n",
    "            self.getDevice(\"wrist_2_joint\"),\n",
    "            self.getDevice(\"wrist_3_joint\")\n",
    "        ]\n",
    "\n",
    "        # 定义活动链接的遮罩\n",
    "        active_links_mask = [False, True, True, True, True, True, False, False]\n",
    "\n",
    "        # 使用ikpy从URDF文件中定义UR5机器人的运动链\n",
    "        self.chain = Chain.from_urdf_file(\"ur5e.urdf\", active_links_mask=active_links_mask)\n",
    "\n",
    "        # 获取每个关节的位置传感器\n",
    "        self.position_sensors = [\n",
    "            self.getDevice(\"shoulder_pan_joint_sensor\"),\n",
    "            self.getDevice(\"shoulder_lift_joint_sensor\"),\n",
    "            self.getDevice(\"elbow_joint_sensor\"),\n",
    "            self.getDevice(\"wrist_1_joint_sensor\"),\n",
    "            self.getDevice(\"wrist_2_joint_sensor\"),\n",
    "            self.getDevice(\"wrist_3_joint_sensor\")\n",
    "        ]\n",
    "\n",
    "        # 启用位置传感器\n",
    "        for sensor in self.position_sensors:\n",
    "            sensor.enable(self.timestep)\n",
    "\n",
    "    def set_pos(self, positions):\n",
    "        # 将每个关节的位置设置为目标位置\n",
    "        for i, pos in enumerate(positions):\n",
    "            self.motors[i].setPosition(pos)\n",
    "        # 步进模拟以应用新的位置\n",
    "        self.step(self.timestep)\n",
    "\n",
    "    def get_pos(self):\n",
    "        # 获取每个关节的当前位置\n",
    "        current_positions = [sensor.getValue() for sensor in self.position_sensors]\n",
    "        return current_positions\n",
    "\n",
    "    def set_joint_positions(self, positions):\n",
    "        # 将每个关节的位置设置为目标位置\n",
    "        for i, pos in enumerate(positions):\n",
    "            self.motors[i].setPosition(pos)\n",
    "        self.step(self.timestep)  # 步进模拟以应用新的位置\n",
    "\n",
    "    def inverse_kinematics(self, target_position):\n",
    "        # 计算目标位置的逆运动学\n",
    "        joint_positions = self.chain.inverse_kinematics(target_position)\n",
    "        return joint_positions[1:7]  # 忽略第一个和最后一个关节，因为它们是固定的\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arm = myUR5()  # 初始化机器人\n",
    "\n",
    "    try:\n",
    "        while True:  # 当RGB-D摄像头打开时\n",
    "            frames = pipe.wait_for_frames()  # 读取帧\n",
    "            color_frame = frames.get_color_frame()\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            if not color_frame or not depth_frame:\n",
    "                continue\n",
    "\n",
    "            # 在处理前，将BGR图像转换为RGB。\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "            results = hands.process(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if results.multi_hand_landmarks:  # 如果找到了手部标记\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                        if id == mp_hands.HandLandmark.WRIST.value:  # 如果是手腕\n",
    "                            wrist_x = int(lm.x * color_image.shape[1])\n",
    "                            wrist_y = int(lm.y * color_image.shape[0])\n",
    "                            wrist_z = depth_image[wrist_y, wrist_x].astype(float)\n",
    "                            if wrist_z==0:\n",
    "                                continue\n",
    "\n",
    "                            original_position = [wrist_x,wrist_y,wrist_z]\n",
    "                            original.append(original_position)\n",
    "\n",
    "\n",
    "                            # 将像素坐标转换为真实世界坐标\n",
    "                            x = (wrist_x - intr.ppx) / intr.fx * wrist_z *1.1\n",
    "                            y = (wrist_y - intr.ppy) / intr.fy * wrist_z \n",
    "                            z = wrist_z * depth_scale\n",
    "\n",
    "                            # 转换坐标系，并可能需要按比例缩放\n",
    "                            scale_factor = 1  # 需要根据实际情况调整\n",
    "                            target_position = [0, 0, .7]\n",
    "                            after.append(target_position)\n",
    "                            joint_positions = arm.inverse_kinematics(target_position)\n",
    "                            arm.set_joint_positions(joint_positions)\n",
    "\n",
    "            # 在图像上绘制手部注释\n",
    "            color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(color_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 显示图像\n",
    "            cv2.imshow('MediaPipe Hands', color_image)\n",
    "            cv2.imshow('Depth Image', depth_colormap)\n",
    "            if cv2.waitKey(5) & 0xFF == 27:  # 如果按下ESC，则退出\n",
    "\n",
    "                outputs = pd.DataFrame({'original': original, 'after': after})\n",
    "                outputs.to_csv(\"positions.csv\",index=False)\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        pipe.stop()  # 关闭RGB-D摄像头\n",
    "        cv2.destroyAllWindows()  # 关闭所有窗口\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [399, 411, 325.0]\n",
       "1      [376, 397, 363.0]\n",
       "2      [399, 413, 366.0]\n",
       "3      [395, 387, 605.0]\n",
       "4      [365, 308, 604.0]\n",
       "             ...        \n",
       "516    [179, 313, 473.0]\n",
       "517    [151, 316, 511.0]\n",
       "518    [127, 319, 577.0]\n",
       "519    [133, 319, 578.0]\n",
       "520     [75, 330, 611.0]\n",
       "Name: original, Length: 521, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"positions.csv\")\n",
    "original = df['original']\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "b = original.to_list()\n",
    "c = [ast.literal_eval(item) for item in b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每列的最大值:  [  507.   413. 65535.]\n",
      "每列的最小值:  [ 75. 271. 299.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设你的3*n数组是a\n",
    "a = np.array(c)\n",
    "\n",
    "# 获取每列的最大值\n",
    "max_values = np.amax(a, axis=0)\n",
    "print('每列的最大值: ', max_values)\n",
    "\n",
    "# 获取每列的最小值\n",
    "min_values = np.amin(a, axis=0)\n",
    "print('每列的最小值: ', min_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
