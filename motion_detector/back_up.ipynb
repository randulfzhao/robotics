{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动作捕捉并储存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基于阈值的方法：\n",
    "你可以定义一些阈值条件，例如当手部关键点的移动速度低于某个阈值时，认为动作已经结束。\n",
    "这种方法的优点是可以更准确地划分动作，但缺点是可能会受到噪声和其他干扰的影响。\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyrealsense2 as rs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def is_start_gesture(hand_landmarks):\n",
    "    # 拇指尖的坐标\n",
    "    thumb_tip = [hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x,\n",
    "                 hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y,\n",
    "                 hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].z]\n",
    "\n",
    "    # 食指尖的坐标\n",
    "    index_finger_tip = [hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x,\n",
    "                        hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y,\n",
    "                        hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z]\n",
    "\n",
    "    # 计算拇指尖和食指尖之间的欧氏距离\n",
    "    distance = np.sqrt(np.sum(np.square(np.subtract(thumb_tip, index_finger_tip))))\n",
    "\n",
    "    # 如果距离小于一定的阈值，认为是\"开始\"手势\n",
    "    return distance < 0.015\n",
    "\n",
    "def is_end_gesture(hand_landmarks):\n",
    "    # 拇指尖的坐标\n",
    "    thumb_tip = [hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x,\n",
    "                 hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y,\n",
    "                 hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].z]\n",
    "\n",
    "    # 小指尖的坐标\n",
    "    pinky_finger_tip = [hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].x,\n",
    "                        hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].y,\n",
    "                        hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].z]\n",
    "\n",
    "    # 计算拇指尖和小指尖之间的欧氏距离\n",
    "    distance = np.sqrt(np.sum(np.square(np.subtract(thumb_tip, pinky_finger_tip))))\n",
    "\n",
    "    # 如果距离小于一定的阈值，认为是\"结束\"手势\n",
    "    return distance < 0.015\n",
    "\n",
    "# 初始化 MediaPipe 和 RealSense\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# 创建文件夹\n",
    "if not os.path.exists('vid'):\n",
    "    os.makedirs('vid')\n",
    "if not os.path.exists('excel'):\n",
    "    os.makedirs('excel')\n",
    "\n",
    "# 初始化视频文件和 DataFrame\n",
    "out = None\n",
    "df_list = []\n",
    "recording = False\n",
    "start_detected = False\n",
    "\n",
    "\n",
    "# 捕获和处理视频帧\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # 将图像转为RGB色彩空间，进行姿势估计\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # 将图像转回BGR色彩空间，进行绘制\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # 判断是否是开始手势\n",
    "                if is_start_gesture(hand_landmarks):\n",
    "                    if not recording and not start_detected:\n",
    "                        print('Start gesture detected.')\n",
    "                        out = cv2.VideoWriter('vid/{}.avi'.format(get_current_time()), cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n",
    "                        df_list = []\n",
    "                        recording = True\n",
    "                        start_detected = True\n",
    "\n",
    "                # 判断是否是结束手势\n",
    "                elif is_end_gesture(hand_landmarks):\n",
    "                    if recording and start_detected:\n",
    "                        print('End gesture detected.')\n",
    "                        out.release()\n",
    "                        df = pd.DataFrame(df_list)\n",
    "                        df.to_csv('excel/{}.csv'.format(get_current_time()), index=False)\n",
    "                        recording = False\n",
    "                        start_detected = False\n",
    "\n",
    "                # 提取关键点值并保存\n",
    "                if recording:\n",
    "                    hand_data = []\n",
    "                    for lm in hand_landmarks.landmark:\n",
    "                        hand_data.append([lm.x, lm.y, lm.z])\n",
    "                    df_list.append(hand_data)\n",
    "\n",
    "        # 将帧写入视频文件\n",
    "        if recording:\n",
    "            out.write(image)\n",
    "\n",
    "        # 显示图像\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "        # 如果按下ESC键，退出循环\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "# 如果在退出循环时仍在录制，保存当前的视频和 DataFrame\n",
    "if recording:\n",
    "    # 如果录制超过0.5秒，才保存\n",
    "    if len(df_list) > 15: # 假设帧率为30fps，那么0.5秒会有15帧\n",
    "        out.release()\n",
    "        df = pd.DataFrame(df_list)\n",
    "        df.to_csv('excel/{}.csv'.format(get_current_time()), index=False)\n",
    "\n",
    "# 释放资源\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类信息读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"检查 'vid' 文件夹内容：\")\n",
    "print(os.listdir('vid'))\n",
    "\n",
    "def rename_file(original_name, append_string):\n",
    "    # 定义视频和csv文件的路径\n",
    "    video_path = os.path.join('vid', original_name)\n",
    "    csv_path = os.path.join('excel', original_name.split('.')[0] + \".csv\")\n",
    "\n",
    "    # 修改视频名\n",
    "    if os.path.exists(video_path):\n",
    "        video_new_name = original_name.split('.')[0] + append_string + \".\" + original_name.split('.')[1]\n",
    "        video_new_path = os.path.join('vid', video_new_name)\n",
    "        os.rename(video_path, video_new_path)\n",
    "\n",
    "    # 修改csv文件名\n",
    "    if os.path.exists(csv_path):\n",
    "        csv_new_name = original_name.split('.')[0] + append_string + \".csv\"\n",
    "        csv_new_path = os.path.join('excel', csv_new_name)\n",
    "        os.rename(csv_path, csv_new_path)\n",
    "\n",
    "def delete_file(file_name):\n",
    "    video_path = os.path.join('vid', file_name)\n",
    "    csv_path = os.path.join('excel', file_name.split('.')[0] + \".csv\")\n",
    "\n",
    "    if os.path.exists(video_path):\n",
    "        os.remove(video_path)\n",
    "        print(f\"视频 '{file_name}' 已删除.\")\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "        print(f\"CSV 文件 '{file_name.split('.')[0]}.csv' 已删除.\")\n",
    "\n",
    "def play_video_with_default_player(video_path):\n",
    "    if os.name == 'nt':  # for Windows\n",
    "        os.startfile(video_path)\n",
    "    else:\n",
    "        opener = \"open\" if sys.platform == \"darwin\" else \"xdg-open\"  # for macOS and Linux\n",
    "        subprocess.call([opener, video_path])\n",
    "\n",
    "def main():\n",
    "    print(\"开始执行 main 函数...\")\n",
    "    for video_name in os.listdir('vid'):\n",
    "        if \"_edited\" not in video_name:  # 更改筛选条件\n",
    "            video_path = os.path.join('vid', video_name)\n",
    "            play_video_with_default_player(video_path)\n",
    "\n",
    "            print(f\"视频 {video_name} 预览完毕!\")\n",
    "            option = input(\"请选择操作：\\n1. 重命名文件\\n2. 删除文件\\n输入选项（1或2）：\")\n",
    "\n",
    "            if option == '1':\n",
    "                append_string = \"_edited\"  # 添加特定后缀表示文件已编辑\n",
    "                rename_file(video_name, append_string)\n",
    "            elif option == '2':\n",
    "                delete_file(video_name)\n",
    "            else:\n",
    "                print(\"无效的选项!\")\n",
    "    print(\"结束 main 函数执行。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def sync_excel_to_vid():\n",
    "    # 获取vid和excel目录下的所有文件\n",
    "    vid_files = os.listdir('vid')\n",
    "    excel_files = os.listdir('excel')\n",
    "    \n",
    "    for excel_file in excel_files:\n",
    "        # 去掉扩展名的文件名\n",
    "        base_name = os.path.splitext(excel_file)[0]\n",
    "        \n",
    "        # 检查是否有相对应的视频文件\n",
    "        matched_video_files = [v for v in vid_files if base_name in v]\n",
    "        \n",
    "        # 如果找到了匹配的视频文件\n",
    "        if matched_video_files:\n",
    "            # 确保只有一个匹配的视频文件\n",
    "            if len(matched_video_files) == 1:\n",
    "                matched_video_file = matched_video_files[0]\n",
    "                new_excel_name = os.path.splitext(matched_video_file)[0] + '.csv'\n",
    "                \n",
    "                # 重命名excel文件\n",
    "                os.rename(os.path.join('excel', excel_file), os.path.join('excel', new_excel_name))\n",
    "            else:\n",
    "                print(f\"在'vid'目录中找到了多个与 '{base_name}' 匹配的文件，无法确定要使用哪一个。\")\n",
    "        # 如果在vid文件夹中找不到匹配的文件\n",
    "        else:\n",
    "            os.remove(os.path.join('excel', excel_file))\n",
    "            print(f\"文件 '{excel_file}' 已从 'excel' 文件夹中删除。\")\n",
    "sync_excel_to_vid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 识别模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drinkWater': 0, 'reachOut': 1, 'getPhone': 2}\n",
      "Epoch [1/10], Average Loss: 1.0916\n",
      "Epoch [2/10], Average Loss: 1.0828\n",
      "Epoch [3/10], Average Loss: 1.0670\n",
      "Epoch [4/10], Average Loss: 1.0537\n",
      "Epoch [5/10], Average Loss: 1.0435\n",
      "Epoch [6/10], Average Loss: 1.0291\n",
      "Epoch [7/10], Average Loss: 1.0289\n",
      "Epoch [8/10], Average Loss: 1.0132\n",
      "Epoch [9/10], Average Loss: 1.0162\n",
      "Epoch [10/10], Average Loss: 1.0126\n",
      "Index: 1, Predicted Label: getPhone\n",
      "Index: 3, Predicted Label: getPhone\n",
      "Index: 4, Predicted Label: getPhone\n",
      "Index: 5, Predicted Label: getPhone\n",
      "Index: 6, Predicted Label: getPhone\n",
      "Index: 7, Predicted Label: getPhone\n",
      "Index: 8, Predicted Label: getPhone\n",
      "Index: 10, Predicted Label: getPhone\n",
      "Index: 11, Predicted Label: getPhone\n",
      "Index: 12, Predicted Label: getPhone\n",
      "Index: 13, Predicted Label: getPhone\n",
      "Index: 0, Predicted Label: reachOut\n",
      "Index: 1, Predicted Label: getPhone\n",
      "Index: 2, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: getPhone\n",
      "Index: 6, Predicted Label: reachOut\n",
      "Index: 7, Predicted Label: reachOut\n",
      "Index: 8, Predicted Label: reachOut\n",
      "Index: 13, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: getPhone\n",
      "Index: 3, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: reachOut\n",
      "Index: 8, Predicted Label: reachOut\n",
      "Index: 11, Predicted Label: reachOut\n",
      "Index: 13, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: reachOut\n",
      "Index: 2, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: reachOut\n",
      "Index: 7, Predicted Label: reachOut\n",
      "Index: 9, Predicted Label: reachOut\n",
      "Index: 10, Predicted Label: reachOut\n",
      "Index: 12, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: reachOut\n",
      "Index: 1, Predicted Label: reachOut\n",
      "Index: 2, Predicted Label: reachOut\n",
      "Index: 3, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: reachOut\n",
      "Index: 6, Predicted Label: reachOut\n",
      "Index: 8, Predicted Label: reachOut\n",
      "Index: 12, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: reachOut\n",
      "Index: 6, Predicted Label: reachOut\n",
      "Index: 9, Predicted Label: reachOut\n",
      "Index: 10, Predicted Label: reachOut\n",
      "Index: 11, Predicted Label: reachOut\n",
      "Index: 12, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: reachOut\n",
      "Index: 1, Predicted Label: reachOut\n",
      "Index: 3, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 6, Predicted Label: reachOut\n",
      "Index: 9, Predicted Label: reachOut\n",
      "Index: 12, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: reachOut\n",
      "Index: 1, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 6, Predicted Label: reachOut\n",
      "Index: 9, Predicted Label: reachOut\n",
      "Index: 10, Predicted Label: reachOut\n",
      "Index: 12, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: reachOut\n",
      "Index: 0, Predicted Label: reachOut\n",
      "Index: 3, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 8, Predicted Label: reachOut\n",
      "Index: 11, Predicted Label: reachOut\n",
      "Index: 13, Predicted Label: reachOut\n",
      "Index: 14, Predicted Label: reachOut\n",
      "Index: 0, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: reachOut\n",
      "Index: 7, Predicted Label: reachOut\n",
      "Index: 8, Predicted Label: reachOut\n",
      "Index: 9, Predicted Label: reachOut\n",
      "Index: 11, Predicted Label: reachOut\n",
      "Index: 0, Predicted Label: reachOut\n",
      "Index: 1, Predicted Label: reachOut\n",
      "Index: 2, Predicted Label: reachOut\n",
      "Index: 3, Predicted Label: reachOut\n",
      "Index: 4, Predicted Label: reachOut\n",
      "Index: 5, Predicted Label: reachOut\n",
      "Index: 10, Predicted Label: reachOut\n",
      "Accuracy on the test data: 53.33%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "\n",
    "# ---------------------------------- 数据预处理 ----------------------------------\n",
    "\n",
    "# functions for data preprocessing\n",
    "def get_label_from_filename(filename):\n",
    "    if 'drinkWater' in filename:\n",
    "        return 'drinkWater'\n",
    "    elif 'reachOut' in filename:\n",
    "        return 'reachOut'\n",
    "    elif 'getPhone' in filename:\n",
    "        return 'getPhone'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_entry(entry):\n",
    "    return np.array(ast.literal_eval(entry))\n",
    "\n",
    "# 将字符串转换为np.array并计算差分\n",
    "def compute_difference(df):\n",
    "    df = df[0].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "    diff = df.diff().dropna()\n",
    "    return diff\n",
    "\n",
    "# 基于差分结果，计算每一行的模\n",
    "def compute_magnitude(diff):\n",
    "    magnitude = diff.apply(lambda x: np.linalg.norm(x))\n",
    "    return magnitude\n",
    "\n",
    "# 根据平均序列长度设置一个阈值，并标记显著性差异\n",
    "def significant_changes(magnitude, average_length):\n",
    "    threshold = 1 / average_length\n",
    "    significant = magnitude > threshold\n",
    "    return significant\n",
    "\n",
    "# 根据显著的差异来选择关键帧\n",
    "def get_keyframes(df, target_length):\n",
    "    diff = compute_difference(df)\n",
    "    magnitude = compute_magnitude(diff)\n",
    "    significant = significant_changes(magnitude, target_length)\n",
    "    \n",
    "    return df.iloc[significant.nlargest(target_length).index]\n",
    "\n",
    "# 加载**训练**数据并进行预处理\n",
    "def preprocess_data(directory):\n",
    "    # Preprocess the data\n",
    "    file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "    # Read the data and get each file's length\n",
    "    lengths = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        lengths.append(len(df))\n",
    "    average_length = int(np.mean(lengths))\n",
    "\n",
    "    processed_data = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        current_length = len(df)\n",
    "\n",
    "        if current_length > average_length:\n",
    "            df = get_keyframes(df, average_length)  # Assuming get_keyframes is a function you've defined elsewhere\n",
    "        processed_data.append(df)\n",
    "        \n",
    "        # Get labels\n",
    "        label = os.path.basename(file).split('.')[0]\n",
    "        label = label.split(\"_\")[-1]\n",
    "        processed_labels.append(label)\n",
    "\n",
    "\n",
    "    for idx, df in enumerate(processed_data):\n",
    "        # Convert df to a nested list\n",
    "        df_list = df.values.tolist()\n",
    "        new_df_list = []  # Will contain the filtered rows\n",
    "\n",
    "        for j in range(len(df_list)):\n",
    "            delete_row = False  # flag to decide whether to delete the row or not\n",
    "\n",
    "            for k in range(len(df_list[j])):\n",
    "                value = df_list[j][k]\n",
    "                if isinstance(value, str) and value.isdigit():  # if it's a string representation of an integer\n",
    "                    delete_row = True\n",
    "                    break  # exit the inner loop early\n",
    "                else:\n",
    "                    df_list[j][k] = ast.literal_eval(value)  # conversion as before\n",
    "\n",
    "            if not delete_row:  # if the flag is still False, keep the row\n",
    "                new_df_list.append(df_list[j])\n",
    "\n",
    "        # Pad the data which is shorter than the average length\n",
    "        while len(new_df_list) < average_length:\n",
    "            new_df_list.append(new_df_list[-1])\n",
    "\n",
    "        processed_data[idx] = new_df_list\n",
    "\n",
    "    return processed_data, processed_labels, average_length\n",
    "\n",
    "# 加载**测试**数据并进行预处理\n",
    "def preprocess_data_test(directory, keyframe):\n",
    "    # Preprocess the data\n",
    "    file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "    # Read the data and get each file's length\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "\n",
    "    processed_data = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        current_length = len(df)\n",
    "\n",
    "        if current_length > keyframe:\n",
    "            df = get_keyframes(df, keyframe)  # Assuming get_keyframes is a function you've defined elsewhere\n",
    "        processed_data.append(df)\n",
    "        \n",
    "        # Get labels\n",
    "        label = os.path.basename(file).split('.')[0]\n",
    "        label = label.split(\"_\")[-1]\n",
    "        processed_labels.append(label)\n",
    "\n",
    "\n",
    "    for idx, df in enumerate(processed_data):\n",
    "        # Convert df to a nested list\n",
    "        df_list = df.values.tolist()\n",
    "        new_df_list = []  # Will contain the filtered rows\n",
    "\n",
    "        for j in range(len(df_list)):\n",
    "            delete_row = False  # flag to decide whether to delete the row or not\n",
    "\n",
    "            for k in range(len(df_list[j])):\n",
    "                value = df_list[j][k]\n",
    "                if isinstance(value, str) and value.isdigit():  # if it's a string representation of an integer\n",
    "                    delete_row = True\n",
    "                    break  # exit the inner loop early\n",
    "                else:\n",
    "                    df_list[j][k] = ast.literal_eval(value)  # conversion as before\n",
    "\n",
    "            if not delete_row:  # if the flag is still False, keep the row\n",
    "                new_df_list.append(df_list[j])\n",
    "\n",
    "        # Pad the data which is shorter than the average length\n",
    "        while len(new_df_list) < keyframe:\n",
    "            new_df_list.append(new_df_list[-1])\n",
    "\n",
    "        processed_data[idx] = new_df_list\n",
    "\n",
    "\n",
    "    return processed_data, processed_labels\n",
    "\n",
    "# ---------------------------------- 数据增强 ----------------------------------\n",
    "\n",
    "# 加噪声\n",
    "def add_noise(points, sigma=0.01):\n",
    "    points_np = np.array(points)\n",
    "    noise = np.random.normal(0, sigma, points_np.shape)\n",
    "    return points_np + noise\n",
    "\n",
    "# 放大缩小\n",
    "def scale(points, scale_factor=None):\n",
    "    points_np = np.array(points)\n",
    "    if scale_factor is None:\n",
    "        scale_factor = np.random.uniform(0.9, 1.1)\n",
    "    return points_np * scale_factor\n",
    "\n",
    "# 旋转\n",
    "def rotate(points, degree_range=10):\n",
    "    points_np = np.array(points)\n",
    "    \n",
    "    if points_np.shape[-1] != 3:  # 只对三维数据执行旋转操作\n",
    "        return points_np\n",
    "    \n",
    "    angle_x = np.radians(np.random.uniform(-degree_range, degree_range))\n",
    "    angle_y = np.radians(np.random.uniform(-degree_range, degree_range))\n",
    "    angle_z = np.radians(np.random.uniform(-degree_range, degree_range))\n",
    "    \n",
    "    rotation_matrix_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(angle_x), -np.sin(angle_x)],\n",
    "        [0, np.sin(angle_x), np.cos(angle_x)]\n",
    "    ])\n",
    "    \n",
    "    rotation_matrix_y = np.array([\n",
    "        [np.cos(angle_y), 0, np.sin(angle_y)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(angle_y), 0, np.cos(angle_y)]\n",
    "    ])\n",
    "    \n",
    "    rotation_matrix_z = np.array([\n",
    "        [np.cos(angle_z), -np.sin(angle_z), 0],\n",
    "        [np.sin(angle_z), np.cos(angle_z), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    rotation_matrix = np.dot(rotation_matrix_z, np.dot(rotation_matrix_y, rotation_matrix_x))\n",
    "    return np.dot(points_np, rotation_matrix.T)\n",
    "\n",
    "# 移动\n",
    "def translate(points, max_translation=0.1):\n",
    "    points_np = np.array(points)\n",
    "    \n",
    "    if points_np.shape[-1] != 3:  # 对非三维数据返回原始数据\n",
    "        return points_np\n",
    "    \n",
    "    dx, dy, dz = np.random.uniform(-max_translation, max_translation, 3)\n",
    "    return points_np + np.array([dx, dy, dz])\n",
    "\n",
    "# 增强某个动作\n",
    "def augment_single_action(action, times=5):\n",
    "    \"\"\"\n",
    "    对单一动作数据进行多次增强。\n",
    "    \n",
    "    参数:\n",
    "    - action: 原始的动作数据\n",
    "    - times: 增强的次数\n",
    "    \n",
    "    返回值:\n",
    "    - 一个增强后的动作数据列表\n",
    "    \"\"\"\n",
    "    augmented_actions = [action]  # 包括原始数据\n",
    "    \n",
    "    for _ in range(times):\n",
    "        augmented_action = []\n",
    "        for keyframe in action:\n",
    "            keyframe = add_noise(keyframe)\n",
    "            keyframe = scale(keyframe)\n",
    "            keyframe = rotate(keyframe)\n",
    "            keyframe = translate(keyframe)\n",
    "            augmented_action.append(keyframe)\n",
    "        augmented_actions.append(augmented_action)\n",
    "    \n",
    "    return augmented_actions\n",
    "\n",
    "# 增强数据集\n",
    "def augment_data_and_labels(data, labels, times=5):\n",
    "    \"\"\"\n",
    "    对整个数据集和标签进行多次增强。\n",
    "    \n",
    "    参数:\n",
    "    - data: 原始的动作数据列表\n",
    "    - labels: 对应的标签列表\n",
    "    - times: 每个动作增强的次数\n",
    "    \n",
    "    返回值:\n",
    "    - 增强后的数据和标签列表\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for action, label in zip(data, labels):\n",
    "        new_actions = augment_single_action(action, times)\n",
    "        augmented_data.extend(new_actions)\n",
    "        augmented_labels.extend([label] * len(new_actions))\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "\n",
    "# ---------------------------- 模型定义及训练工具函数 ----------------------------\n",
    "\n",
    "# 定义LSTM模型：LSTM架构的浅层RNN\n",
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 检测输入数据的结构是否符合预设\n",
    "def check_shape(data, desired_shape):\n",
    "    # 每个动作的关键帧长度是否相同\n",
    "    for i, action in enumerate(data):\n",
    "        if len(action) != desired_shape[0]:\n",
    "            print(f\"Action at index {i} has {len(action)} keyframes instead of {desired_shape[0]}.\")\n",
    "\n",
    "        # 每个关键帧的关键点数量是否相同\n",
    "        for j, keyframe in enumerate(action):\n",
    "            if len(keyframe) != desired_shape[1]:\n",
    "                print(f\"Keyframe {j} in action at index {i} has {len(keyframe)} keypoints instead of {desired_shape[1]}.\")\n",
    "\n",
    "            # 每个关键点的数据输入是否是三维的\n",
    "            for k, keypoint in enumerate(keyframe):\n",
    "                        try:\n",
    "                            if len(keypoint) != desired_shape[2]:\n",
    "                                print(f\"Keypoint {k} in keyframe {j} of action at index {i} has a shape of {len(keypoint)} instead of {desired_shape[2]}.\")\n",
    "                        except:\n",
    "                            print(f\"Keypoint {k} in keyframe {j} of action at index {i} is {keyframe} instead of list of length {desired_shape[2]}.\")\n",
    "\n",
    "\n",
    "# ----------------------------- 模型训练与评估函数 ------------------------------\n",
    "\n",
    "# 检测错误分类的结果\n",
    "def get_wrongly_classified_info(outputs, labels):\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    wrong_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
    "    wrong_predictions = predicted[wrong_indices]\n",
    "    return wrong_indices.tolist(), wrong_predictions.tolist()\n",
    "\n",
    "# 分割训练集和测试集\n",
    "def split_data_for_training(data, labels, test_size=0.1):\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=test_size)\n",
    "    return train_data, val_data, train_labels, val_labels\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, device, early_stop_patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs - 1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter == early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# body part code\n",
    "# 1. data preprocessing\n",
    "train_directory = \"excel\"\n",
    "data, labels, keyframe = preprocess_data(train_directory)\n",
    "\n",
    "# data augmentation\n",
    "augmented_data, augmented_labels = augment_data_and_labels(data, labels, times=5)\n",
    "\n",
    "# ckeck shape of input training\n",
    "desired_shape = (keyframe, 21, 3)\n",
    "check_shape(data,desired_shape)\n",
    "\n",
    "# 数据处理：转换为 [batch, seq_len, input_size] 的格式\n",
    "data = [[[coord for keypoint in frame for coord in keypoint] for frame in action] for action in data]\n",
    "\n",
    "# 创建label到整数的映射\n",
    "label_to_int = {label: idx for idx, label in enumerate(set(labels))}\n",
    "int_to_label = {idx: label for label, idx in label_to_int.items()}\n",
    "\n",
    "# 打印编码情况\n",
    "print(label_to_int)\n",
    "\n",
    "# 将字符串标签编码为整数\n",
    "encoded_labels = [label_to_int[label] for label in labels]\n",
    "\n",
    "# 将嵌套的列表结构转换为torch tensor\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(encoded_labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# 2. 定义LSTM模型\n",
    "input_dim = 63  # 展平后的关键点维度\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_to_int)\n",
    "num_layers = 2\n",
    "\n",
    "model = ActionClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 4. 训练模型，并记录训练误差, 同时也记录错分类的数据索引和预测值\n",
    "num_epochs = 10\n",
    "train_errors = []\n",
    "\n",
    "wrongly_classified_train_info = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_error = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_error += loss.item()\n",
    "\n",
    "        # Collect wrongly classified information\n",
    "        wrong_indices, wrong_predictions = get_wrongly_classified_info(outputs, labels)\n",
    "        wrongly_classified_train_info.extend(zip(wrong_indices, wrong_predictions))\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_error /= len(train_loader)\n",
    "    train_errors.append(epoch_error)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_error:.4f}')\n",
    "\n",
    "# 打印错分类的训练数据信息\n",
    "print(\"Error Information for Training samples:\")\n",
    "for idx, prediction in wrongly_classified_train_info:\n",
    "    print(f\"Index: {idx}, Original Label is: {labels[idx]} Predicted Label: {int_to_label[prediction]}\")\n",
    "\n",
    "with open('train_errors.txt', 'w') as f:\n",
    "    for error in train_errors:\n",
    "        f.write(f\"{error}\\n\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# 5. 测试函数\n",
    "def evaluate_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "# 6. 为新的CSV文件测试模型\n",
    "test_directory = 'data_test'\n",
    "test_data, test_labels = preprocess_data_test(test_directory,keyframe)\n",
    "encoded_test_labels = [label_to_int[label] for label in test_labels]\n",
    "test_data = [[[coord for keypoint in frame for coord in keypoint] for frame in action] for action in test_data]\n",
    "\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(encoded_test_labels, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "\n",
    "# Collect wrongly classified test data information\n",
    "wrongly_classified_test_info = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        outputs = model(inputs)\n",
    "        wrong_indices, wrong_predictions = get_wrongly_classified_info(outputs, labels)\n",
    "        wrongly_classified_test_info.extend(zip(wrong_indices, wrong_predictions))\n",
    "\n",
    "# 打印错分类的测试数据信息\n",
    "print(\"Error Information for Testing Samples:\")\n",
    "for idx, prediction in wrongly_classified_train_info:\n",
    "    print(f\"Index: {idx}, Original Label is: {test_labels[idx]} Predicted Label: {int_to_label[prediction]}\")\n",
    "\n",
    "for idx, prediction in wrongly_classified_test_info:\n",
    "    print(f\"Index: {idx}, Predicted Label: {int_to_label[prediction]}\")\n",
    "\n",
    "print(f'Accuracy on the test data: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $data$: dataset of actions\n",
    "* $data[i]$: $i^{th}$ action\n",
    "* $data[i][j]$: $j^{th}$ keyframe of $i^{th}$ action\n",
    "* $data[i][j][k]$: the $k^{th}$ key point's information for the $j^{th}$ keyframe of $i^{th}$ action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drinkWater': 0, 'reachOut': 1, 'getPhone': 2}\n",
      "Epoch [1/10], Average Loss: 1.0989\n",
      "Epoch [2/10], Average Loss: 1.0858\n",
      "Epoch [3/10], Average Loss: 1.0697\n",
      "Epoch [4/10], Average Loss: 1.0591\n",
      "Epoch [5/10], Average Loss: 1.0450\n",
      "Epoch [6/10], Average Loss: 1.0305\n",
      "Epoch [7/10], Average Loss: 1.0150\n",
      "Epoch [8/10], Average Loss: 1.0121\n",
      "Epoch [9/10], Average Loss: 1.0170\n",
      "Epoch [10/10], Average Loss: 1.0097\n",
      "Training complete.\n",
      "Accuracy on the test data: 53.33%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 定义LSTM模型\n",
    "class ActionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# functions for data preprocessing\n",
    "def get_label_from_filename(filename):\n",
    "    if 'drinkWater' in filename:\n",
    "        return 'drinkWater'\n",
    "    elif 'reachOut' in filename:\n",
    "        return 'reachOut'\n",
    "    elif 'getPhone' in filename:\n",
    "        return 'getPhone'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_entry(entry):\n",
    "    return np.array(ast.literal_eval(entry))\n",
    "\n",
    "# 将字符串转换为np.array并计算差分\n",
    "def compute_difference(df):\n",
    "    df = df[0].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "    diff = df.diff().dropna()\n",
    "    return diff\n",
    "\n",
    "# 基于差分结果，计算每一行的模\n",
    "def compute_magnitude(diff):\n",
    "    magnitude = diff.apply(lambda x: np.linalg.norm(x))\n",
    "    return magnitude\n",
    "\n",
    "# 根据平均序列长度设置一个阈值，并标记显著性差异\n",
    "def significant_changes(magnitude, average_length):\n",
    "    threshold = 1 / average_length\n",
    "    significant = magnitude > threshold\n",
    "    return significant\n",
    "\n",
    "# 根据显著的差异来选择关键帧\n",
    "def get_keyframes(df, target_length):\n",
    "    diff = compute_difference(df)\n",
    "    magnitude = compute_magnitude(diff)\n",
    "    significant = significant_changes(magnitude, target_length)\n",
    "    \n",
    "    return df.iloc[significant.nlargest(target_length).index]\n",
    "\n",
    "def preprocess_data(directory):\n",
    "    # Preprocess the data\n",
    "    file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "    # Read the data and get each file's length\n",
    "    lengths = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        lengths.append(len(df))\n",
    "    average_length = int(np.mean(lengths))\n",
    "\n",
    "    processed_data = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        current_length = len(df)\n",
    "\n",
    "        if current_length > average_length:\n",
    "            df = get_keyframes(df, average_length)  # Assuming get_keyframes is a function you've defined elsewhere\n",
    "        processed_data.append(df)\n",
    "        \n",
    "        # Get labels\n",
    "        label = os.path.basename(file).split('.')[0]\n",
    "        label = label.split(\"_\")[-1]\n",
    "        processed_labels.append(label)\n",
    "\n",
    "\n",
    "    for idx, df in enumerate(processed_data):\n",
    "        # Convert df to a nested list\n",
    "        df_list = df.values.tolist()\n",
    "        new_df_list = []  # Will contain the filtered rows\n",
    "\n",
    "        for j in range(len(df_list)):\n",
    "            delete_row = False  # flag to decide whether to delete the row or not\n",
    "\n",
    "            for k in range(len(df_list[j])):\n",
    "                value = df_list[j][k]\n",
    "                if isinstance(value, str) and value.isdigit():  # if it's a string representation of an integer\n",
    "                    delete_row = True\n",
    "                    break  # exit the inner loop early\n",
    "                else:\n",
    "                    df_list[j][k] = ast.literal_eval(value)  # conversion as before\n",
    "\n",
    "            if not delete_row:  # if the flag is still False, keep the row\n",
    "                new_df_list.append(df_list[j])\n",
    "\n",
    "        # Pad the data which is shorter than the average length\n",
    "        while len(new_df_list) < average_length:\n",
    "            new_df_list.append(new_df_list[-1])\n",
    "\n",
    "        processed_data[idx] = new_df_list\n",
    "\n",
    "\n",
    "    return processed_data, processed_labels, average_length\n",
    "\n",
    "\n",
    "# functions for data augmentation\n",
    "def add_noise(points, sigma=0.01):\n",
    "    points_np = np.array(points)\n",
    "    noise = np.random.normal(0, sigma, points_np.shape)\n",
    "    return points_np + noise\n",
    "\n",
    "def scale(points, scale_factor=None):\n",
    "    points_np = np.array(points)\n",
    "    if scale_factor is None:\n",
    "        scale_factor = np.random.uniform(0.9, 1.1)\n",
    "    return points_np * scale_factor\n",
    "\n",
    "def rotate(points, degree_range=10):\n",
    "    points_np = np.array(points)\n",
    "    \n",
    "    if points_np.shape[-1] != 3:  # 只对三维数据执行旋转操作\n",
    "        return points_np\n",
    "    \n",
    "    angle_x = np.radians(np.random.uniform(-degree_range, degree_range))\n",
    "    angle_y = np.radians(np.random.uniform(-degree_range, degree_range))\n",
    "    angle_z = np.radians(np.random.uniform(-degree_range, degree_range))\n",
    "    \n",
    "    rotation_matrix_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(angle_x), -np.sin(angle_x)],\n",
    "        [0, np.sin(angle_x), np.cos(angle_x)]\n",
    "    ])\n",
    "    \n",
    "    rotation_matrix_y = np.array([\n",
    "        [np.cos(angle_y), 0, np.sin(angle_y)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(angle_y), 0, np.cos(angle_y)]\n",
    "    ])\n",
    "    \n",
    "    rotation_matrix_z = np.array([\n",
    "        [np.cos(angle_z), -np.sin(angle_z), 0],\n",
    "        [np.sin(angle_z), np.cos(angle_z), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    rotation_matrix = np.dot(rotation_matrix_z, np.dot(rotation_matrix_y, rotation_matrix_x))\n",
    "    return np.dot(points_np, rotation_matrix.T)\n",
    "\n",
    "def translate(points, max_translation=0.1):\n",
    "    points_np = np.array(points)\n",
    "    \n",
    "    if points_np.shape[-1] != 3:  # 对非三维数据返回原始数据\n",
    "        return points_np\n",
    "    \n",
    "    dx, dy, dz = np.random.uniform(-max_translation, max_translation, 3)\n",
    "    return points_np + np.array([dx, dy, dz])\n",
    "\n",
    "def augment_single_action(action, times=5):\n",
    "    \"\"\"\n",
    "    对单一动作数据进行多次增强。\n",
    "    \n",
    "    参数:\n",
    "    - action: 原始的动作数据\n",
    "    - times: 增强的次数\n",
    "    \n",
    "    返回值:\n",
    "    - 一个增强后的动作数据列表\n",
    "    \"\"\"\n",
    "    augmented_actions = [action]  # 包括原始数据\n",
    "    \n",
    "    for _ in range(times):\n",
    "        augmented_action = []\n",
    "        for keyframe in action:\n",
    "            keyframe = add_noise(keyframe)\n",
    "            keyframe = scale(keyframe)\n",
    "            keyframe = rotate(keyframe)\n",
    "            keyframe = translate(keyframe)\n",
    "            augmented_action.append(keyframe)\n",
    "        augmented_actions.append(augmented_action)\n",
    "    \n",
    "    return augmented_actions\n",
    "\n",
    "def augment_data_and_labels(data, labels, times=5):\n",
    "    \"\"\"\n",
    "    对整个数据集和标签进行多次增强。\n",
    "    \n",
    "    参数:\n",
    "    - data: 原始的动作数据列表\n",
    "    - labels: 对应的标签列表\n",
    "    - times: 每个动作增强的次数\n",
    "    \n",
    "    返回值:\n",
    "    - 增强后的数据和标签列表\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for action, label in zip(data, labels):\n",
    "        new_actions = augment_single_action(action, times)\n",
    "        augmented_data.extend(new_actions)\n",
    "        augmented_labels.extend([label] * len(new_actions))\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "\n",
    "# check the input shape of training data\n",
    "def check_shape(data,desired_shape):\n",
    "    # 遍历每个动作\n",
    "    for i, action in enumerate(data):\n",
    "        # 检查动作的关键帧数量\n",
    "        if len(action) != desired_shape[0]:\n",
    "            print(f\"Action at index {i} has {len(action)} keyframes instead of {desired_shape[0]}.\")\n",
    "        else:\n",
    "            # 如果关键帧数量符合，则进一步检查每个关键帧的关键点数量\n",
    "            for j, keyframe in enumerate(action):\n",
    "                if len(keyframe) != desired_shape[1]:\n",
    "                    print(f\"Keyframe {j} in action at index {i} has {len(keyframe)} keypoints instead of {desired_shape[1]}.\")\n",
    "                else:\n",
    "                    # 最后检查每个关键点的维度\n",
    "                    for k, keypoint in enumerate(keyframe):\n",
    "                        try:\n",
    "                            if len(keypoint) != desired_shape[2]:\n",
    "                                print(f\"Keypoint {k} in keyframe {j} of action at index {i} has a shape of {len(keypoint)} instead of {desired_shape[2]}.\")\n",
    "                        except:\n",
    "                            print(f\"Keypoint {k} in keyframe {j} of action at index {i} is {keyframe} instead of list of length {desired_shape[2]}.\")\n",
    "\n",
    "\n",
    "# prepare data for testing\n",
    "def preprocess_data_test(directory, keyframe):\n",
    "    # Preprocess the data\n",
    "    file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "    # Read the data and get each file's length\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "\n",
    "    processed_data = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        current_length = len(df)\n",
    "\n",
    "        if current_length > keyframe:\n",
    "            df = get_keyframes(df, keyframe)  # Assuming get_keyframes is a function you've defined elsewhere\n",
    "        processed_data.append(df)\n",
    "        \n",
    "        # Get labels\n",
    "        label = os.path.basename(file).split('.')[0]\n",
    "        label = label.split(\"_\")[-1]\n",
    "        processed_labels.append(label)\n",
    "\n",
    "\n",
    "    for idx, df in enumerate(processed_data):\n",
    "        # Convert df to a nested list\n",
    "        df_list = df.values.tolist()\n",
    "        new_df_list = []  # Will contain the filtered rows\n",
    "\n",
    "        for j in range(len(df_list)):\n",
    "            delete_row = False  # flag to decide whether to delete the row or not\n",
    "\n",
    "            for k in range(len(df_list[j])):\n",
    "                value = df_list[j][k]\n",
    "                if isinstance(value, str) and value.isdigit():  # if it's a string representation of an integer\n",
    "                    delete_row = True\n",
    "                    break  # exit the inner loop early\n",
    "                else:\n",
    "                    df_list[j][k] = ast.literal_eval(value)  # conversion as before\n",
    "\n",
    "            if not delete_row:  # if the flag is still False, keep the row\n",
    "                new_df_list.append(df_list[j])\n",
    "\n",
    "        # Pad the data which is shorter than the average length\n",
    "        while len(new_df_list) < keyframe:\n",
    "            new_df_list.append(new_df_list[-1])\n",
    "\n",
    "        processed_data[idx] = new_df_list\n",
    "\n",
    "\n",
    "    return processed_data, processed_labels\n",
    "\n",
    "\n",
    "# body part code\n",
    "# 1. data preprocessing\n",
    "train_directory = \"excel\"\n",
    "data, labels, keyframe = preprocess_data(train_directory)\n",
    "\n",
    "# data augmentation\n",
    "augmented_data, augmented_labels = augment_data_and_labels(data, labels, times=5)\n",
    "\n",
    "# ckeck shape of input training\n",
    "desired_shape = (keyframe, 21, 3)\n",
    "check_shape(data,desired_shape)\n",
    "\n",
    "# 数据处理：转换为 [batch, seq_len, input_size] 的格式\n",
    "data = [[[coord for keypoint in frame for coord in keypoint] for frame in action] for action in data]\n",
    "\n",
    "# 创建label到整数的映射\n",
    "label_to_int = {label: idx for idx, label in enumerate(set(labels))}\n",
    "int_to_label = {idx: label for label, idx in label_to_int.items()}\n",
    "\n",
    "# 打印编码情况\n",
    "print(label_to_int)\n",
    "\n",
    "# 将字符串标签编码为整数\n",
    "encoded_labels = [label_to_int[label] for label in labels]\n",
    "\n",
    "# 将嵌套的列表结构转换为torch tensor\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(encoded_labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# 2. 定义LSTM模型\n",
    "input_dim = 63  # 展平后的关键点维度\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_to_int)\n",
    "num_layers = 2\n",
    "\n",
    "model = ActionClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 4. 训练模型，并记录训练误差\n",
    "num_epochs = 10\n",
    "train_errors = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_error = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_error += loss.item()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_error /= len(train_loader)\n",
    "    train_errors.append(epoch_error)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_error:.4f}')\n",
    "\n",
    "with open('train_errors.txt', 'w') as f:\n",
    "    for error in train_errors:\n",
    "        f.write(f\"{error}\\n\")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# 5. 测试函数\n",
    "def evaluate_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "# 6. 为新的CSV文件测试模型\n",
    "test_directory = 'data_test'\n",
    "test_data, test_labels = preprocess_data_test(test_directory,keyframe)\n",
    "encoded_test_labels = [label_to_int[label] for label in test_labels]\n",
    "test_data = [[[coord for keypoint in frame for coord in keypoint] for frame in action] for action in test_data]\n",
    "\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(encoded_test_labels, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy on the test data: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
